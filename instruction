1. Installation
首先克隆所需要的代码库
HUGS：https://github.com/apple/ml-hugs
Animatable_Gaussian: https://github.com/lizhe00/AnimatableGaussians
    2. Preparing the datasets and models
1.下载人物数据集
AvatarReX（https://github.com/lizhe00/AnimatableGaussians/blob/master/AVATARREX_DATASET.md），
Thuman4.0，（https://github.com/ZhengZerong/THUman4.0-Dataset）
ActorsHQ（自己从谷歌下）

2.人物训练根据https://github.com/lizhe00/AnimatableGaussians进行，用他的pre来做，路径无效，就自己创建文件夹，按照路径来

3.如果需要换动作序列

python main_avatar_joint.py -c configs/mvhn_101010/avatar.yaml --mode=test

更改avatar.yaml文件里pose- sequence的npz文件地址，L69 save ply 更改,并且对npz文件的trans和oreinted 做适配
python trans_npz_all.py \
  --src pose_00.npz \
  --tgt_npz 01_newformat.npz \
  --mode align_first_frame \
  --out_npz 01_newformat_rooted.npz


run main_avatar_joint.py to make 3d joint +ply, then use addline2joint sequence.py to make 3d skeleton
小结：训练了mvhumannet 30个新人物

3.场景/home/fzhi/fzt/3dgs_pipeline/animatable_dataset/scene/数据集里面是处理好的场景点云ply文件
处理新的3dgs点云文件
3. combine human gaussian and scene gaussian
1. 拿到人物渲染之后的数据集
1.1joint :

python save_joints_trans_sequence.py \
  --input_dir /home/fzhi/fzt/3dgs_pipeline/animatable_3DGS/AnimatableGaussians/test_results/avatarrex_zzr/avatar/thuman4__pose_00_free_view/batch_700000/pca_20_sigma_2.00/joints/ \
  --output_dir /home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/output_combine/joint/

1.2joint line to 

python add_line2joint_sequence.py \
  --in_dir /home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/output_combine/joint/npy/transformed/ \
  --out_dir /home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/output_combine/joint/pt/ \
  --start 0 --end 99

ps:如果帧数不是 0~99？怎么查？
看看 .npy 的最大编号，然后替换 –end。

2.1人物的trans_matrix在 /home/fzhi/fzt/3dgs_pipeline/animatable_dataset/human_trans/trans.md
then use python file in /home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/scripts/transform_human_sequence.py

python scripts/djr/djr_position_2/transform_human_sequence_djr.py \
--input_dir /mnt/data_hdd/fzhi/human_data/101010/test_results/pca_20_sigma_2.00/posed_gaussians/ \
--output_dir /mnt/data_hdd/fzhi/mid/101010/postion1 \
--output_format both \
--start_frame 00000000 \
--end_frame 00000001

acJ9oz79KciB_lbJqh9XAW86MQp1OmJnMgk.01.0z126tiwc

backward_0.5m
d：3kMewPtyLoxyWDhpNbfRr286MQp1OmJnMwk.01.0z1niukww

2.2在hugs/ml-hugs/scripts/transform_human_sequence.py更改transform_matrix

python hugs/renderer/render_sequence_from1camera.py \
  --human_pt_dir "/mnt/data_hdd/fzhi/mid/101010/postion1/pt" \
  --scene_pt "/home/fzhi/fzt/3dgs_pipeline/animatable_dataset/scenefixed/djr/djr_3dgs.pt" \
  --output_dir "/mnt/data_hdd/fzhi/output/101010/djr/positon2" \
  --start_frame 00000000\
  --end_frame 00000001\
  --camera_json "/home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/camera/djr/djr_positon_1/djr_top.json" \
  --render_mode human_scene

python hugs/renderer/render_sequence_firstcamera.py \
  --human_pt_dir "/mnt/data_hdd/zhiyw/test/first/pt/" \
  --scene_pt "/home/zhiyw/Desktop/animatable_dataset/scene/playroom/playroom.pt" \
  --output_dir "/home/zhiyw/Desktop/test/" \
  --start_frame 00000000\
  --end_frame 00000001\
  --camera_json "/home/zhiyw/Desktop/ml-hugs/mip_camera/lab_train_camera_params//playroom.json" \
  --render_mode scene


3.人物点云和场景点云融合用ml-hugs/hugs/renderer/render_sequence_firstcamera.py(单个摄像头)或者hugs/hugs/renderer/gs_renderer.py(多个摄像头， 有几个摄像头渲染几张图片)
python hugs/renderer/gs_renderer_debugV1.py        --human_pt output/gs_out/human_full_sh2.pt        --scene_pt output/gs_out/scene_gs_out.pt        --camera_json camera_params/lab/train/camera_params.json        --render_mode human_scene        --out_png rendered_images/human_scene_finish.png

旋转矩阵：
djr_position1:
T_total = np.array([
    [ 0.11431113,  0.56197536,  0.81921715, -0.43538997],
    [-0.14897871,  0.82499462, -0.54515064, -1.21867311],
    [-0.98221099, -0.05972914,  0.17802842,  0.78487867],
    [ 0.0,         0.0,         0.0,         1.0        ]
]) 
djr_postion2:
T_total = np.array([
    [ 0.097174286842,  0.776527106762,  0.622545421124, -0.031882703304],
    [-0.883775293827,  0.354985624552, -0.304838359356,  0.666232705116],
    [-0.457709938288, -0.520567834377,  0.720770955086,  4.502630233765],
    [ 0.0,             0.0,             0.0,             1.0            ]
])
playroom:
total = np.array([[ 1.1431149  -1.14361141  1.55017474 -0.12080942]
 [-0.8450672  -1.91755666 -0.79147898  2.99686062]
 [ 1.7311127  -0.18091512 -1.4100071  -1.04102581]
 [ 0.          0.          0.          1.        ]])
 
T = np.array([
[ 0.302041769028, 0.004976983648, 0.105574645102, -9.022824287415],
[ 0.028119388968, 0.304340690374, -0.094794824719, 3.952184200287],
[-0.101882658899, 0.098752155900, 0.286823898554, -6.463224887848],
[ 0.0, 0.0, 0.0, 1.0]
])
T2 = np.array([ [0.277242637, 0.004076329, -0.159749540, 0.474825714], [-0.064292293, 0.295708864, -0.104032584, 6.120443290], [0.146297743,0.122227906, 0.257016186, -8.623109620], [0.0, 0.0, 0.0, 1.0] ])
 

4.使用摄像头的四元数生成摄像头json文件
摄像头的四元数path是在neuman/dataset/lab/sparse
这一步的使用是选取camera.txt加上对应的image.txt作为run_extract_camera.py的input,需要使用那个场景，就把那个场景的image_name.txt改成image.txt,比如说,如果要生成playroom的摄像头json 文件，那就把image_playroom.txt改成image.txt

使用脚本调整摄像头位置
python adjust_camera_position_advanced.py --input images_djr.txt --height 2.5 --output images_height_2.5m.txt
python adjust_camera_position_advanced.py --input images_height_2.5m.txt --forward 0.5 --output images_forward_0.5m.txt
python adjust_camera_position_advanced.py --input images_height_2.5m.txt --forward -0.5 --output images_backward_0.5m.txt
python adjust_camera_position_advanced.py --input images_height_2.5m.txt --right -0.5 --output images_left_0.5m.txt
python adjust_camera_position_advanced.py --input images_height_2.5m.txt --right 0.5 --output images_right_0.5m.txt


python run_extract_camera.py  --seq lab --split train  --output output

（我已经生成好的json摄像机文件在ml-hugs/mip-camera/lab_train_camera_params）

scene 转换
在Blender中调整至正确状态，然后使用Python脚本控制台。

import bpy
import numpy as np

obj = bpy.context.active_object
M = np.array(obj.matrix_world, dtype=np.float64)

np.set_printoptions(precision=12, suppress=True)
print(M)

然后再transformer_3dgs_ply 文件中修改ply场景的旋转矩阵：
transformer_3dgs_ply cmd:
python ply_to_pt.py --in_ply scene_out.ply --out_pt scene_out.pt

然后转换成pt文件：

python ply2pt_mip_nerf_scene.py \
  --in_ply /home/fzhi/fzt/3dgs_pipeline/animatable_dataset/rotation/djr_3dgs_aligned.ply \
  --out_pt /home/fzhi/fzt/3dgs_pipeline/animatable_dataset/rotation/djr_aligned.pt



在配置ml-hug，以及anim3dgs环境的时候解决cuda 11.7 和12.0不匹配的问题
export CUDA_HOME=$HOME/cuda-11.7
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
 
更改libjpg.so path
export LIBRARY_PATH=$CONDA_PREFIX/lib:$LIBRARY_PATH
export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH
 
Exr读取
export OPENCV_IO_ENABLE_OPENEXR=1
 
Ml-hugs的路径设置
export PYTHONPATH=$(pwd):$PYTHONPATH



远程训练新的数据集：

0）准备：目录结构确认

假设你的新数据在：

/mnt/data_hdd/fzhi/dataset/103105/
里面至少要有（名字可能略不同，但语义要一致）：

cameras/ 里每个相机一个子目录或文件，包含 camera.npz 或相机参数

smplx_params_new/（或你的版本叫 smplx/）里逐帧 *.npz

多视角图片/掩码（训练阶段一定会用到：images/ masks/ 等）
check ： 
ls /mnt/data2/fzhi/103105
ls /mnt/data2/fzhi/103105/cameras | head
ls /mnt/data2/fzhi/103028/smplx_params_new | head

1.
预处理：
conda activate anima3dgs
cd /home/fzhi/fzt/AnimatableGaussians

python gen_data/preprocess_mvhnpp.py --mvhn_subj_path /mnt/data_hdd/fzhi/dataset/102231 

python gen_data/preprocess_mvhnpp.py --mvhn_subj_path /mnt/data2/fzhi/103105/

生成calibration文件

check：
ls /mnt/data_hdd/fzhi/dataset/102231/calibrations.json
ls /mnt/data_hdd/fzhi/dataset/102231/smpl_params.npz
 
ls /mnt/data2/fzhi/103105/calibrations.json
ls /mnt/data2/fzhi/103105/smpl_params.npz

2.为 101223 新建 configs（只需要改 3 个地方）
1.复制一份 101010 的配置
cd ~/fzt/3dgs_pipeline/animatable_3DGS/anima_yu/AnimatableGaussians/configs
cp -r mvhn_101412 mvhn_103105

必改 1：subject_name
subject_name: 103105

必改 2：data_dir
data_dir: /mnt/data_hdd/fzhi/dataset/103105

必改 3：输出目录（你希望结果放哪）

net_ckpt_dir: /mnt/data_hdd/fzhi/dataset/103105/results/template

# avatar.yaml 里：
pretrained_dir: /mnt/data_hdd/fzhi/dataset/103105/avatar/pretrained
net_ckpt_dir: /mnt/data_hdd/fzhi/dataset/103105/avatar

然后 
tmux new -s train103105
conda activate anima3dgs
cd ~/fzt/3dgs_pipeline/animatable_3DGS/anima_yu/AnimatableGaussians
运行脚本 记得修改subject name！

chmod +x train_mvhnpp.sh
./train_mvhnpp.sh mvhn_103105


可能会遇到的环境编译问题：

conda activate anim3dgs
git clone https://github.com/mkazhdan/PoissonRecon.git
conda install -c conda-forge libpng jpeg zlib -y
make pointinterpolant
ls $CONDA_PREFIX/lib | egrep "libpng|libjpeg|libz" | head
export LIBRARY_PATH=$CONDA_PREFIX/lib:$LIBRARY_PATH                                                                                                                                                                                                                                                                                         
export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH                                                                                                                                                 
export CPATH=$CONDA_PREFIX/include:$CPATH                                                                                                                                                                 
export C_INCLUDE_PATH=$CONDA_PREFIX/include:$C_INCLUDE_PATH                                                                                                                                               
export CPLUS_INCLUDE_PATH=$CONDA_PREFIX/include:$CPLUS_INCLUDE_PATH  



在 主目录下，mkdir -p bins
cp PoissonRecon/Bin/Linux/PointInterpolant ./bins/
chmod +x ./bins/PointInterpolant

然后vim ./gen_data/gen_weight_volume.py
solve(smpl_model.lbs_weights.shape[-1], "./bins/PointInterpolant")


https://github.com/LetianHuang/op43dgs 

# detach: Ctrl+B D


查看：
tmux ls
tmux attach -t train10100


tmux new -s mvhn101010
然后再跑脚本

export CUDA_VISIBLE_DEVICES=0
在那台电脑运行这个。

ssh -L 6006:localhost:6008 fzhi@dst-swarm.etit.tu-chemnitz.de


1已完成数据集：
101477
101010
101020 miko
101223
101412 miko
101883 
101207 miko
102231
<<<<<<< HEAD
100834
100840
100931
100990
101029
101032
=======
101177 miko
103028
103105


python tools/analysis_tools/browse_dataset.py configs/VisDrone.py \
                                              --show-interval 3

tensorboard --logdir /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/output/train/ --port 6006

yolo

dataset transfer
python tools/dataset_converters/bisdrone2coco.py --img-dir ./data/cat/images \
                                                --labels-dir ./data/cat/labels \
                                                --out ./data/cat/annotations/annotations_all.json

python tools/analysis_tools/browse_coco_json.py --img-dir /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/train/images \
                                                --ann-file /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/train/coco_label/train.json

python tools/analysis_tools/browse_dataset.py configs/visdrone.py \
                                              --show-interval 3
train
      python tools/train.py configs/visdrone.py
test:

python tools/test.py configs/visdrone.py \
                    /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/output/train/epoch_92.pth \
                     --show-dir /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/output/test

