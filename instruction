1. Installation
首先克隆所需要的代码库
HUGS：https://github.com/apple/ml-hugs
Animatable_Gaussian: https://github.com/lizhe00/AnimatableGaussians
    2. Preparing the datasets and models
1.下载人物数据集
AvatarReX（https://github.com/lizhe00/AnimatableGaussians/blob/master/AVATARREX_DATASET.md），
Thuman4.0，（https://github.com/ZhengZerong/THUman4.0-Dataset）
ActorsHQ（自己从谷歌下）

2.人物训练根据https://github.com/lizhe00/AnimatableGaussians进行，用他的pre来做，路径无效，就自己创建文件夹，按照路径来

3.如果需要换动作序列

python main_avatar_joint.py -c configs/mvhn_101010/avatar.yaml --mode=test

更改avatar.yaml文件里pose- sequence的npz文件地址，L69 save ply 更改,并且对npz文件的trans和oreinted 做适配

python trans_npz_all.py \
  --src pose_00.npz \
  --tgt_npz 01_newformat.npz \
  --mode align_first_frame \
  --out_npz 01_newformat_rooted.npz


run main_avatar_joint.py to make 3d joint +ply, then use addline2joint sequence.py to make 3d skeleton
小结：训练了mvhumannet 30个新人物

3.场景/home/fzhi/fzt/3dgs_pipeline/animatable_dataset/scene/数据集里面是处理好的场景点云ply文件
处理新的3dgs点云文件
3. combine human gaussian and scene gaussian
1. 拿到人物渲染之后的数据集
1.1joint :

python save_joints_trans_sequence.py \
  --input_dir /home/fzhi/fzt/3dgs_pipeline/animatable_3DGS/AnimatableGaussians/test_results/avatarrex_zzr/avatar/thuman4__pose_00_free_view/batch_700000/pca_20_sigma_2.00/joints/ \
  --output_dir /home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/output_combine/joint/

1.2joint line to 

python add_line2joint_sequence.py \
  --in_dir /home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/output_combine/joint/npy/transformed/ \
  --out_dir /home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/output_combine/joint/pt/ \
  --start 0 --end 99

ps:如果帧数不是 0~99？怎么查？
看看 .npy 的最大编号，然后替换 –end。

2.1 align:
python scripts/counter/transform_human_sequence.py \
--input_dir /mnt/data_hdd/fzhi/human_data/101010/test_results/pca_20_sigma_2.00/posed_gaussians/ \
--output_dir /mnt/data_hdd/fzhi/mid/101010/counter/position_1 \
--output_format both \
--start_frame 00000000 \
--end_frame 00000001

python scripts/djr/p2/transform_human_sequence.py \
--input_dir /mnt/data_hdd/fzhi/human_data/101010/test_results/pca_20_sigma_2.00/posed_gaussians/ \
--output_dir /mnt/data_hdd/fzhi/mid/101010/djr/p2 \
--output_format both \
--start_frame 00000000 \
--end_frame 00000001


render:

python hugs/renderer/render_sequence_from1camera.py \
  --human_pt_dir "/mnt/data_hdd/fzhi/mid/101010/djr/p2/pt/"\
  --scene_pt "/home/fzhi/fzt/3dgs_pipeline/animatable_dataset/scenefixed/djr/djr.pt" \
  --output_dir "/mnt/data_hdd/fzhi/output/101010/djr/testp2/top/" \
  --start_frame 00000000\
  --end_frame 00000001\
  --camera_json "/home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/camera/djr_topp2.json"\
  --render_mode human_scene


  python hugs/renderer/render_sequence_from1camera.py \
  --human_pt_dir "/mnt/data_hdd/fzhi/mid/101010/djr/position_1/pt" \
  --scene_pt "/home/fzhi/fzt/3dgs_pipeline/animatable_dataset/rotation/counter_aligned_cut.pt" \
  --output_dir "/mnt/data_hdd/fzhi/output/101010/counter/test/top/" \
  --start_frame 00000000\
  --end_frame 00000001\
  --camera_json "/home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/camera/counter/p1/counter_top.json"\
  --render_mode scene

3.人物点云和场景点云融合用ml-hugs/hugs/renderer/render_sequence_firstcamera.py(单个摄像头)或者hugs/hugs/renderer/gs_renderer.py(多个摄像头， 有几个摄像头渲染几张图片)
python hugs/renderer/gs_renderer_debugV1.py        --human_pt output/gs_out/human_full_sh2.pt        --scene_pt output/gs_out/scene_gs_out.pt        --camera_json camera_params/lab/train/camera_params.json        --render_mode human_scene        --out_png rendered_images/human_scene_finish.png

旋转矩阵：
djr_position1:
T = np.array([
    [ 0.072728075087,  0.858601033688,  0.507459282875, -1.072966337204],
    [-0.995899140835,  0.035067219287,  0.083397984505,  1.208601236343],
    [ 0.053810410202, -0.511443614960,  0.857630372047,  0.289357781410],
    [ 0.0,             0.0,             0.0,             1.0]
], dtype=np.float64)

djr_postion2:
T_total = np.array([
    [ 0.097174286842,  0.776527106762,  0.622545421124, -0.031882703304],
    [-0.883775293827,  0.354985624552, -0.304838359356,  0.666232705116],
    [-0.457709938288, -0.520567834377,  0.720770955086,  4.502630233765],
    [ 0.0,             0.0,             0.0,             1.0            ]
])
djr_position3:
T = np.array([
    [-0.151482775807,  0.849509418011,  0.505357980728, -0.746910810471],
    [-0.958956837654, -0.002324781846, -0.283542603254,  0.873314142227],
    [-0.239697277546, -0.527568280697,  0.814995050430, -2.874177217484],
    [ 0.000000000000,  0.000000000000,  0.000000000000,  1.000000000000]
])

playroom:
    T_TOTAL = np.array([
        [ 0.95676806, -1.12771127,  1.68239787,  0.5641625 ],
        [ 0.00870305, -1.85836058, -1.25060843,  2.2911702 ],
        [ 2.02536921,  0.54070739, -0.78937747, -2.34121266],
        [ 0.0,         0.0,         0.0,         1.0]], dtype=np.float64)
Room:
position1:
Transformation matrix applied:

[[-2.44379453 -0.47741441  0.38774337  3.69274177]
 [ 0.10870355 -1.89897366 -1.65302248  1.1510081 ]
 [ 0.60535333 -1.58630894  1.86214197 -0.69644767]
 [ 0.          0.          0.          1.        ]]


position 2:
Transformation matrix applied:
[[-2.44379453 -0.47741441  0.38774337  4.42851045]
 [ 0.10870355 -1.89897366 -1.65302248  1.1510081 ]
 [ 0.60535333 -1.58630894  1.86214197  0.86278731]
 [ 0.          0.          0.          1.        ]]

counter:
Transformation matrix applied:
total=  np.array([[-1.58511592 -0.71332374  1.17837939  1.3652371 ]
 [-0.29006622 -1.58334779 -1.34865539  2.7719918 ]
 [ 1.34657729 -1.18075365  1.09660876  2.63152217]
 [ 0.          0.          0.          1.        ]])

garden:
Transformation matrix applied:
Transformation matrix applied:
[[-1.97442593  0.530018   -0.48033743 -0.65253485]
 [ 0.14701865 -1.07939381 -1.79535371  1.1591649 ]
 [-0.7000206  -1.72162446  0.97774322  1.99992636]
 [ 0.          0.          0.          1.        ]]
 new :
 [[-2.06746281 -0.05552876 -0.36402598 -0.04313992]
 [ 0.28645282 -1.54703895 -1.39090455  1.44365869]
 [-0.23139405 -1.41900942  1.53064371  1.61558577]
 [ 0.          0.          0.          1.        ]]





4.使用摄像头的四元数生成摄像头json文件,fisheye     op43dgs/submodules/diff-gaussian-rasterization-fisheye

使用脚本调整摄像头高度
python adjust_camera_position_advanced.py --input images_aligned.txt --height 2.5 --output images_top.txt

set_camera_tx_ty_tz_over_head.py 根据人物trans输出相机位置
python set_camera_tx_ty_tz_over_head.py \
  --images_txt images_djr.txt \
  --human_ply /mnt/data_hdd/fzhi/mid/101010/djr/p2/ply/00000000.ply \
  --out_images_txt mages_djr_overhead.txt \
  --image_id 0 \
  --distance 2.5 \
  --mode head \
  --up_axis 2 \
  --top_percent 0.5

room:
/mnt/data_hdd/fzhi/mid/101010/room/p2

python set_camera_tx_ty_tz_over_head.py \
  --images_txt images.txt \
  --human_ply /mnt/data_hdd/fzhi/mid/101010/djr/p3/ply/00000000.ply \
  --out_images_txt images_overhead.txt \
  --image_id 0 \
  --distance 2.5 \
  --mode head \
  --up_axis 2 \
  --top_percent 0.5


chmod +x extract_camera_from_sparse_images.sh
./extract_camera_from_sparse_images.sh \
images_overhead.txt \
djr_topp3.json \
/home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/camera/

./extract_camera_from_sparse_images.sh \
images_forward.txt \
djr_forward.json \
/home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/camera/

./extract_camera_from_sparse_images.sh \
images_backward.txt \
djr_backward.json \
/home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/camera/

./extract_camera_from_sparse_images.sh \
images_left.txt \
djr_left.json \
/home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/camera/

./extract_camera_from_sparse_images.sh \
images_right.txt \
djr_right.json \
/home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/camera/

###python run_extract_camera.py  --seq lab --split train  --output output


scene 转换
在Blender中调整至正确状态，然后使用Python脚本控制台。

import bpy
import numpy as np

obj = bpy.context.active_object
M = np.array(obj.matrix_world, dtype=np.float64)

np.set_printoptions(precision=12, suppress=True)
print(M)
counter:
 [[ 1.              0.              0.              0.            ]
  [ 0.              0.796962857246  0.60402828455   0.            ]
  [ 0.             -0.60402828455   0.796962857246  0.            ]
 [ 0.              0.              0.              1.            ]]

 garden
M = np.array([
    [ 0.999988853931, -0.001920162351,  0.004311107099,  0.0],
    [ 0.0,             0.913487672806,  0.406866401434,  0.0],
    [-0.004719392397, -0.406861871481,  0.913477480412,  0.0],
    [ 0.0,             0.0,             0.0,             1.0]
], dtype=np.float64)
room:

print(M)
#~ [[ 0.9982213974    0.017244892195 -0.057067498565  0.            ]
#~  [ 0.013747725636  0.864861726761  0.501821935177  0.            ]
#~  [ 0.05800935626  -0.501713931561  0.863086342812  0.            ]
#~  [ 0.              0.              0.              1.            ]]
#~
playroom:


然后再transformer_3dgs_ply 文件中修改ply场景的旋转矩阵：
transformer_3dgs_ply cmd:
python transform_3dgs_ply.py --in_ply playroom.ply --out_ply playroom_aligned.ply

然后转换成pt文件：

python ply2pt_mip_nerf_scene.py \
  --in_ply /home/fzhi/fzt/3dgs_pipeline/animatable_dataset/rotation/counter_aligned_cut.ply \
  --out_pt /home/fzhi/fzt/3dgs_pipeline/animatable_dataset/rotation/counter_aligned_cut.pt







在配置ml-hug，以及anim3dgs环境的时候解决cuda 11.7 和12.0不匹配的问题
export CUDA_HOME=$HOME/cuda-11.7
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
 
更改libjpg.so path
export LIBRARY_PATH=$CONDA_PREFIX/lib:$LIBRARY_PATH
export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH
 
Exr读取
export OPENCV_IO_ENABLE_OPENEXR=1
 
Ml-hugs的路径设置
export PYTHONPATH=$(pwd):$PYTHONPATH



远程训练新的数据集：

0）准备：目录结构确认

假设你的新数据在：

/mnt/data_hdd/fzhi/dataset/103105/
里面至少要有（名字可能略不同，但语义要一致）：

cameras/ 里每个相机一个子目录或文件，包含 camera.npz 或相机参数

smplx_params_new/（或你的版本叫 smplx/）里逐帧 *.npz

多视角图片/掩码（训练阶段一定会用到：images/ masks/ 等）
check ： 
ls /mnt/data2/fzhi/103105
ls /mnt/data2/fzhi/103105/cameras | head
ls /mnt/data2/fzhi/103028/smplx_params_new | head

1.
预处理：
conda activate anima3dgs
cd /home/fzhi/fzt/AnimatableGaussians

python gen_data/preprocess_mvhnpp.py --mvhn_subj_path /mnt/data_hdd/fzhi/dataset/102231 

python gen_data/preprocess_mvhnpp.py --mvhn_subj_path /mnt/data2/fzhi/103105/

生成calibration文件

check：
ls /mnt/data_hdd/fzhi/dataset/102231/calibrations.json
ls /mnt/data_hdd/fzhi/dataset/102231/smpl_params.npz
 
ls /mnt/data2/fzhi/103105/calibrations.json
ls /mnt/data2/fzhi/103105/smpl_params.npz

2.为 101223 新建 configs（只需要改 3 个地方）
1.复制一份 101010 的配置
cd ~/fzt/3dgs_pipeline/animatable_3DGS/anima_yu/AnimatableGaussians/configs
cp -r mvhn_101412 mvhn_103105

必改 1：subject_name
subject_name: 103105

必改 2：data_dir
data_dir: /mnt/data_hdd/fzhi/dataset/103105

必改 3：输出目录（你希望结果放哪）

net_ckpt_dir: /mnt/data_hdd/fzhi/dataset/103105/results/template

# avatar.yaml 里：
pretrained_dir: /mnt/data_hdd/fzhi/dataset/103105/avatar/pretrained
net_ckpt_dir: /mnt/data_hdd/fzhi/dataset/103105/avatar

然后 
tmux new -s train103105
conda activate anima3dgs
cd ~/fzt/3dgs_pipeline/animatable_3DGS/anima_yu/AnimatableGaussians
运行脚本 记得修改subject name！

chmod +x train_mvhnpp.sh
./train_mvhnpp.sh mvhn_103105


可能会遇到的环境编译问题：

conda activate anim3dgs
git clone https://github.com/mkazhdan/PoissonRecon.git
conda install -c conda-forge libpng jpeg zlib -y
make pointinterpolant
ls $CONDA_PREFIX/lib | egrep "libpng|libjpeg|libz" | head
export LIBRARY_PATH=$CONDA_PREFIX/lib:$LIBRARY_PATH                                                                                                                                                                                                                                                                                         
export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH                                                                                                                                                 
export CPATH=$CONDA_PREFIX/include:$CPATH                                                                                                                                                                 
export C_INCLUDE_PATH=$CONDA_PREFIX/include:$C_INCLUDE_PATH                                                                                                                                               
export CPLUS_INCLUDE_PATH=$CONDA_PREFIX/include:$CPLUS_INCLUDE_PATH  



在 主目录下，mkdir -p bins
cp PoissonRecon/Bin/Linux/PointInterpolant ./bins/
chmod +x ./bins/PointInterpolant

然后vim ./gen_data/gen_weight_volume.py
solve(smpl_model.lbs_weights.shape[-1], "./bins/PointInterpolant")


https://github.com/LetianHuang/op43dgs 

# detach: Ctrl+B D


查看：
tmux ls
tmux attach -t train10100


tmux new -s mvhn101010
然后再跑脚本

export CUDA_VISIBLE_DEVICES=0
在那台电脑运行这个。

ssh -L 6006:localhost:6008 fzhi@dst-swarm.etit.tu-chemnitz.de


1已完成数据集：
101477
101010
101020 miko
101223
101412 miko
101883 
101207 miko
102231
<<<<<<< HEAD
100834
100840
100931
100990
101029
101032
=======
101177 miko
103028
103105


python tools/analysis_tools/browse_dataset.py configs/VisDrone.py \
                                              --show-interval 3

tensorboard --logdir /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/output/train/ --port 6006

yolo

dataset transfer
python tools/dataset_converters/bisdrone2coco.py --img-dir ./data/cat/images \
                                                --labels-dir ./data/cat/labels \
                                                --out ./data/cat/annotations/annotations_all.json

python tools/analysis_tools/browse_coco_json.py --img-dir /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/train/images \
                                                --ann-file /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/train/coco_label/train.json

python tools/analysis_tools/browse_dataset.py configs/visdrone.py \
                                              --show-interval 3
train
      python tools/train.py configs/visdrone.py
test:

python tools/test.py configs/visdrone.py \
                    /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/output/train/epoch_92.pth \
                     --show-dir /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/output/test

