1. Installation
首先克隆所需要的代码库
HUGS：https://github.com/apple/ml-hugs
Animatable_Gaussian: https://github.com/lizhe00/AnimatableGaussians
    2. Preparing the datasets and models
1.下载人物数据集
AvatarReX（https://github.com/lizhe00/AnimatableGaussians/blob/master/AVATARREX_DATASET.md），
Thuman4.0，（https://github.com/ZhengZerong/THUman4.0-Dataset）
ActorsHQ（自己从谷歌下）

2.人物训练根据https://github.com/lizhe00/AnimatableGaussians进行，用他的pre来做，路径无效，就自己创建文件夹，按照路径来

3.如果需要换动作序列

python main_avatar_joint.py -c configs/mvhn_101010/avatar.yaml --mode=test

更改avatar.yaml文件里pose- sequence的npz文件地址，L69 save ply 更改,并且对npz文件的trans和oreinted 做适配
trans 替换
python trans_npz_all_fix.py \
  --src pose_00.npz \
  --tgt_npz 06_13_poses.npz \
  --mode align_first_frame \
  --inplace_mode xy \
  --out_npz 06_13_poses_inplace.npz
#inplace: xy，z可以动。xyz，完全不动，没有行动轨迹
#subframe:

python subsample_npz_stride.py \
  --input 06_13_poses_inplace.npz \
  --stride 5
#check annotation 是否好用

python check_annotation_pipeline.py \
  --motion 06_13_poses.npz \
  --joints /mnt/data_hdd/fzhi/human_data/101010/06_13_poses_inplace_stride5/test_results/pca_20_sigma_2.00/joints/00000000.npy \
  --scene djr_p1
SMPL-H
156 = 52 × 3

python check_annotation_pipeline.py \
  --motion 06_13_poses.npz \
  --joints_dir /mnt/data_hdd/fzhi/human_data/101010/06_13_poses_inplace_stride5/test_results/pca_20_sigma_2.00/joints \
  --joints_pattern "*.npy" \
  --scene djr_p1 \
  --frame 0
#--camera camera/room_top.json \




run main_avatar_joint.py to make 3d joint +ply, then use addline2joint sequence.py to make 3d skeleton

小结：训练了mvhumannet 30个新人物

3.场景/home/fzhi/fzt/3dgs_pipeline/animatable_dataset/scene/数据集里面是处理好的场景点云ply文件
处理新的3dgs点云文件
3. combine human gaussian and scene gaussian
1. 拿到人物渲染之后的数据集
1.1joint :


python save_joints_trans_sequence.py \
  --input_dir /mnt/data_hdd/fzhi/human_data/101010/06_13_poses_inplace_stride5/test_results/pca_20_sigma_2.00/joints \
  --output_dir /mnt/data_hdd/fzhi/output/101010/06_13_poses_inplace_stride5/djr/p1 \
  --scene djr_p1
  --motion 06_13_poses_fix_xyz_stride5.npz #可省略，让他自己推断出来。

1.2joint line to 

python add_line2joint_sequence.py \
  --in_dir /home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/output_combine/joint/npy/transformed/ \
  --out_dir /home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/output_combine/joint/pt/ \
  --start 0 --end 99

ps:如果帧数不是 0~99？怎么查？
看看 .npy 的最大编号，然后替换 –end。
/mnt/data_hdd/fzhi/human_data/101010/06_13_poses_trans_stride5/test_results/pca_20_sigma_2.00/posed_gaussians/
2.1 align:
python scripts/human_trans/djr/p1/transform_human_sequence.py \
--input_dir /mnt/data_hdd/fzhi/human_data/101010/06_13_poses_inplace_stride5/test_results/pca_20_sigma_2.00/posed_gaussians/ \
--output_dir /mnt/data_hdd/fzhi/mid/101010/06_13_poses_inplace_stride5/djr/p1 \
--output_format both \
--start_frame 00000000 \
--end_frame 00000099

#/mnt/data_hdd/fzhi/mid/101010/06_13_poses_trans/djr/p1
#render:

python hugs/renderer/render_sequence_from1camera.py \
  --human_pt_dir "/mnt/data_hdd/fzhi/mid/101010/06_13_poses_inplace_stride5/djr/p1/pt/"\
  --scene_pt "/home/fzhi/fzt/3dgs_pipeline/animatable_dataset/scenefixed/djr/djr.pt" \
  --output_dir "/mnt/data_hdd/fzhi/output/101010/06_13_poses_inplace_stride5/djr/p1/top/" \
  --start_frame 00000000\
  --end_frame 00000099\
  --camera_json "/home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/camera/djr/p1/top.json"\
  --render_mode human_scene



3.人物点云和场景点云融合用ml-hugs/hugs/renderer/render_sequence_firstcamera.py(单个摄像头)或者hugs/hugs/renderer/gs_renderer.py(多个摄像头， 有几个摄像头渲染几张图片)
python hugs/renderer/gs_renderer_debugV1.py        --human_pt output/gs_out/human_full_sh2.pt        --scene_pt output/gs_out/scene_gs_out.pt        --camera_json camera_params/lab/train/camera_params.json        --render_mode human_scene        --out_png rendered_images/human_scene_finish.png

旋转矩阵：

djr_position1 
    T_total = np.array([
        [ 0.072728075087,  0.858601033688,  0.507459282875, -1.072966337204],
        [-0.995899140835,  0.035067219287,  0.083397984505,  1.208601236343],
        [ 0.053810410202, -0.511443614960,  0.857630372047,  0.289357781410],
        [ 0.0,             0.0,             0.0,             1.0]], dtype=np.float64)

djr_position2 
T_total = np.array([
    [ 0.097174286842,  0.776527106762,  0.622545421124, -0.031882703304],
    [-0.883775293827,  0.354985624552, -0.304838359356,  0.666232705116],
    [-0.457709938288, -0.520567834377,  0.720770955086,  4.502630233765],
    [ 0.0,             0.0,             0.0,             1.0            ]
])

 p3:
 T_total =  np.array([
    [-0.067112356512,  0.846463411812,  0.528200308463, -0.841071732405],
    [-0.968525243892,  0.071911939214, -0.238301321409,  0.866211993675],
    [-0.239697277546, -0.527568280697,  0.814995050430, -2.874177217484],
    [ 0.000000000000,  0.000000000000,  0.000000000000,  1.000000000000]
])

playroom:
    T_TOTAL = np.array([
        [ 0.95676806, -1.12771127,  1.68239787,  0.5641625 ],
        [ 0.00870305, -1.85836058, -1.25060843,  2.2911702 ],
        [ 2.02536921,  0.54070739, -0.78937747, -2.34121266],
        [ 0.0,         0.0,         0.0,         1.0]], dtype=np.float64)
Room:

room_position1 = np.array([
    [-2.44379453, -0.47741441,  0.38774337,  3.69274177],
    [ 0.10870355, -1.89897366, -1.65302248,  1.1510081 ],
    [ 0.60535333, -1.58630894,  1.86214197, -0.69644767],
    [ 0.0,         0.0,         0.0,         1.0]
])

room_position2 = np.array([
    [-2.44379453, -0.47741441,  0.38774337,  4.42851045],
    [ 0.10870355, -1.89897366, -1.65302248,  1.1510081 ],
    [ 0.60535333, -1.58630894,  1.86214197,  0.86278731],
    [ 0.0,         0.0,         0.0,         1.0]
])


counter:
counter_T = np.array([
    [-1.58511592, -0.71332374,  1.17837939,  1.3652371 ],
    [-0.29006622, -1.58334779, -1.34865539,  2.7719918 ],
    [ 1.34657729, -1.18075365,  1.09660876,  2.63152217],
    [ 0.0,         0.0,         0.0,         1.0]
])

garden:
Transformation matrix applied:
 garden_T = np.array([
    [-2.06746281, -0.05552876, -0.36402598, -0.04313992],
    [ 0.28645282, -1.54703895, -1.39090455,  1.44365869],
    [-0.23139405, -1.41900942,  1.53064371,  1.61558577],
    [ 0.0,         0.0,         0.0,         1.0]
])



4.使用摄像头的四元数生成摄像头json文件,fisheye    git:https://github.com/LetianHuang/op43dgs 
submodule: op43dgs/submodules/diff-gaussian-rasterization-fisheye

set_camera_tx_ty_tz_over_head.py 根据人物trans输出相机位置

python set_camera_tx_ty_tz_over_head.py \
  --images_txt images_djr.txt \
  --human_ply /mnt/data_hdd/fzhi/mid/101010/djr/p3/ply/00000000.ply \
  --out_images_txt images_djr_centerp3.txt \
  --image_id 0 \
  --distance 3 \
  --mode center \
  --top_percent 0.5

#前后左右的生成摄像头

python shift_camera_4dirs_images_txt.py \
  --images_txt images_room_centerp2.txt \
  --out_dir cam_shift_txt \
  --image_id 0 \
  --shift 0.5 \
  --world_up_axis auto \
  --human_ply /mnt/data_hdd/fzhi/mid/101010/room/p2/ply/00000000.ply \
  --auto_axes

SPARSE_DIR="/home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/sparse/test/cam_shift_txt"
CAM_DIR="/home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/camera"

cd "$SPARSE_DIR"


chmod +x extract_camera_from_sparse_images.sh
./extract_camera_from_sparse_images.sh \
images_room_centerp2_backward_0.50m.txt \
images_room_centerp2_backward.json \
/home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/camera/

./extract_camera_from_sparse_images.sh \
images_room_centerp2_forward_0.50m.txt \
images_room_centerp2_forward.json \
/home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/camera

./extract_camera_from_sparse_images.sh \
images_room_centerp2_left_0.50m.txt \
images_room_centerp2_left.json \
/home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/camera

./extract_camera_from_sparse_images.sh \
images_room_centerp2_right_0.50m.txt \
images_room_centerp2_right.json \
/home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/camera

./extract_camera_from_sparse_images.sh \
images_room_centerp2.txt \
images_room_centerp2.json \
/home/fzhi/fzt/3dgs_pipeline/ml_hug/ml-hugs/camera


###python run_extract_camera.py  --seq lab --split train  --output output

render所有的djr的场景
chmod +x render_all_p_all_views.sh
./render_all_p_all_views.sh


scene 转换
在Blender中调整至正确状态，然后使用Python脚本控制台。

import bpy
import numpy as np

obj = bpy.context.active_object
M = np.array(obj.matrix_world, dtype=np.float64)

np.set_printoptions(precision=12, suppress=True)
print(M)
counter:
 [[ 1.              0.              0.              0.            ]
  [ 0.              0.796962857246  0.60402828455   0.            ]
  [ 0.             -0.60402828455   0.796962857246  0.            ]
 [ 0.              0.              0.              1.            ]]

 garden
M = np.array([
    [ 0.999988853931, -0.001920162351,  0.004311107099,  0.0],
    [ 0.0,             0.913487672806,  0.406866401434,  0.0],
    [-0.004719392397, -0.406861871481,  0.913477480412,  0.0],
    [ 0.0,             0.0,             0.0,             1.0]
], dtype=np.float64)
room:

print(M)
#~ [[ 0.9982213974    0.017244892195 -0.057067498565  0.            ]
#~  [ 0.013747725636  0.864861726761  0.501821935177  0.            ]
#~  [ 0.05800935626  -0.501713931561  0.863086342812  0.            ]
#~  [ 0.              0.              0.              1.            ]]
#~
playroom:


然后再transformer_3dgs_ply 文件中修改ply场景的旋转矩阵：
transformer_3dgs_ply cmd:
python transform_3dgs_ply.py --in_ply playroom.ply --out_ply playroom_aligned.ply

然后转换成pt文件：

python ply2pt_mip_nerf_scene.py \
  --in_ply /home/fzhi/fzt/3dgs_pipeline/animatable_dataset/rotation/counter_aligned_cut.ply \
  --out_pt /home/fzhi/fzt/3dgs_pipeline/animatable_dataset/rotation/counter_aligned_cut.pt







在配置ml-hug，以及anim3dgs环境的时候解决cuda 11.7 和12.0不匹配的问题
export CUDA_HOME=$HOME/cuda-11.7
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
 
更改libjpg.so path
export LIBRARY_PATH=$CONDA_PREFIX/lib:$LIBRARY_PATH
export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH
 
Exr读取
export OPENCV_IO_ENABLE_OPENEXR=1
 
Ml-hugs的路径设置
export PYTHONPATH=$(pwd):$PYTHONPATH



远程训练新的数据集：

0）准备：目录结构确认

假设你的新数据在：

/mnt/data_hdd/fzhi/dataset/103105/
里面至少要有（名字可能略不同，但语义要一致）：

cameras/ 里每个相机一个子目录或文件，包含 camera.npz 或相机参数

smplx_params_new/（或你的版本叫 smplx/）里逐帧 *.npz

多视角图片/掩码（训练阶段一定会用到：images/ masks/ 等）
check ： 
ls /mnt/data2/fzhi/103105
ls /mnt/data2/fzhi/103105/cameras | head
ls /mnt/data2/fzhi/103028/smplx_params_new | head

1.
预处理：
conda activate anima3dgs
cd /home/fzhi/fzt/AnimatableGaussians

python gen_data/preprocess_mvhnpp.py --mvhn_subj_path /mnt/data2/fzhi/101089/

生成calibration文件

check：

ls /mnt/data2/fzhi/101089/calibrations.json
ls /mnt/data2/fzhi/101089/smpl_params.npz

2.为 101223 新建 configs（只需要改 3 个地方）
1.复制一份 101010 的配置
cd /home/fzhi/fzt/AnimatableGaussians/configs/
cp -r mvhn_101020 mvhn_101089

必改 1：subject_name
subject_name: 101089

必改 2：data_dir
data_dir: /mnt/data_hdd/fzhi/dataset/101089

必改 3：输出目录（你希望结果放哪）

net_ckpt_dir: /mnt/data_hdd/fzhi/dataset/101089/results/template

# avatar.yaml 里：
pretrained_dir: /mnt/data_hdd/fzhi/dataset/101089/avatar/pretrained
net_ckpt_dir: /mnt/data_hdd/fzhi/dataset/101089/avatar

然后 
tmux new -s train_101089
conda activate anima3dgs
cd /home/fzhi/fzt/AnimatableGaussians
运行脚本 记得修改subject name！

chmod +x train_mvhnpp.sh
./train_mvhnpp.sh mvhn_101089


可能会遇到的环境编译问题：

conda activate anim3dgs
git clone https://github.com/mkazhdan/PoissonRecon.git
conda install -c conda-forge libpng jpeg zlib -y
make pointinterpolant
ls $CONDA_PREFIX/lib | egrep "libpng|libjpeg|libz" | head
export LIBRARY_PATH=$CONDA_PREFIX/lib:$LIBRARY_PATH                                                                                                                                                                                                                         
export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH                                                                                                                                                 
export CPATH=$CONDA_PREFIX/include:$CPATH                                                                                                                                                                 
export C_INCLUDE_PATH=$CONDA_PREFIX/include:$C_INCLUDE_PATH                                                                                                                                               
export CPLUS_INCLUDE_PATH=$CONDA_PREFIX/include:$CPLUS_INCLUDE_PATH  



在 主目录下，mkdir -p bins
cp PoissonRecon/Bin/Linux/PointInterpolant ./bins/
chmod +x ./bins/PointInterpolant

然后vim ./gen_data/gen_weight_volume.py
solve(smpl_model.lbs_weights.shape[-1], "./bins/PointInterpolant")


# detach: Ctrl+B D


查看：
tmux ls
tmux attach -t train10100


然后再跑脚本

export CUDA_VISIBLE_DEVICES=0

在那台电脑运行这个。

ssh -L 6006:localhost:6008 fzhi@dst-swarm.etit.tu-chemnitz.de


1已完成数据集：
101477
101010
101020 miko
101223
101412 miko
101883 
101207 miko
102231
<<<<<<< HEAD
100834
100840
100931
100990
101029
101032
=======
101177 miko
101089








python tools/analysis_tools/browse_dataset.py configs/VisDrone.py \
                                              --show-interval 3

tensorboard --logdir /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/output/train/ --port 6006

yolo

dataset transfer
python tools/dataset_converters/bisdrone2coco.py --img-dir ./data/cat/images \
                                                --labels-dir ./data/cat/labels \
                                                --out ./data/cat/annotations/annotations_all.json

python tools/analysis_tools/browse_coco_json.py --img-dir /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/train/images \
                                                --ann-file /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/train/coco_label/train.json

python tools/analysis_tools/browse_dataset.py configs/visdrone.py \
                                              --show-interval 3
train
      python tools/train.py configs/visdrone.py
test:

python tools/test.py configs/visdrone.py \
                    /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/output/train/epoch_92.pth \
                     --show-dir /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/output/test

python tools/analysis_tools/optimize_anchors.py configs/custom_dataset/yolov5_s-v61_syncbn_fast_1xb32-100e_cat.py \
                                                --algorithm v5-k-means \
                                                --input-shape 640 640 \
                                                --prior-match-thr 4.0 \
                                                --out-dir work_dirs/dataset_analysis_cat


数据EDA：

python eda_coco.py \
  --ann /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/data/annotations/train.json \
  --imgdir /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/data/images/\
  --out runs/eda_train

  yolo 评估
  bbox_mAP：
  python tools/test.py \
    configs/visdrone.py \
   /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/output/train/epoch_92.pth
  fps：
 python tools/analysis_tools/benchmark.py \
  configs/visdrone.py \
  /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/output/train/epoch_100.pth
  可视化：
  python tools/test.py \
  configs/visdrone.py \
  /mnt/data_hdd/fzhi/track_data/VisDrone/image_detection/output/train/epoch_100.pth \
  --show-dir debug_vis

